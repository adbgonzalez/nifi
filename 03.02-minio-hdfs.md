# 03.02 Procesadores para MinIO / S3 e HDFS en Apache NiFi

Este documento recolle fichas completas dos procesadores necesarios para integrar NiFi con **MinIO/S3** e **HDFS**, incluíndo usos típicos no curso, boas prácticas e erros comúns.

---

# ================================
# 2. Procesadores MinIO / S3
# ================================

## 2.1. PutS3Object

### 1. Descrición
`PutS3Object` escribe un FlowFile como obxecto nun bucket S3 compatible, incluíndo MinIO. Permite definir bucket, clave, metadatos e configuración de seguridade mediante un Controller Service.

### 2. Función principal
- Subir FlowFiles como obxectos a MinIO ou Amazon S3.
- Definir rutas/keys dinámicas para cada FlowFile.
- Integrar fluxos NiFi con lagoas de datos baseadas en S3.

### 3. Propiedades principais
| Propiedade | Tipo | Descrición |
|------------|------|-------------|
| Bucket     | EL   | Nome do bucket → `#{bucket}` ou `${bucket}` |
| Object Key | EL   | Nome do ficheiro → `${filename}` |
| Endpoint Override URL | URL | Para MinIO: `http://minio:9000` |
| Access Key / Secret | CS | Defínense nun Controller Service |
| Use Path Style Access | Boolean | En MinIO debe ser `true` |

### 4. Relacións
| Relación  | Significado |
|-----------|-------------|
| success   | Subida correcta |
| failure   | Erro na subida |
| retry     | Pode reintentarse |

### 5. Exemplo típico (MinIO)
Previo en `UpdateAttribute`:
```
bucket = nifi-test
filename = datos-${now():format('yyyyMMdd-HHmmss')}.json
```
PutS3Object:
- Bucket = `${bucket}`
- Object Key = `${filename}`
- Endpoint = `http://minio:9000`

### 6. Erros comúns
| Erro | Causa |
|------|-------|
| InvalidRequest: AWS4-HMAC-SHA256 required | Non se usa S3ConfigurationService correcto |
| AccessDenied | credenciais mal configuradas |
| Bucket non existe | MinIO require crealo antes |

### 7. Boas prácticas
- Usar **Parameter Contexts** para endpoint, bucket e credenciais.
- Poñer todos os fallos a **terminate** para evitar loops infinitos.
- Non usar rutas relativas: sempre definir Object Key con extensión.

---

## 2.2. FetchS3Object

### 1. Descrición
Descarga un obxecto dun bucket S3/MinIO e convérteo nun FlowFile.

### 2. Función principal
- Ler datos almacenados en MinIO.
- Integrar pipelines onde NiFi consume datos dun lago S3.

### 3. Propiedades principais
| Propiedade | Tipo | Descrición |
|------------|------|-------------|
| Bucket     | EL   | Nome do bucket |
| Object Key | EL   | Rutas ou patróns (un ficheiro por vez) |

### 4. Relacións
| Relación | Significado |
|----------|-------------|
| success  | Obxecto descargado |
| not.found | O ficheiro non existe |
| failure  | Erro na descarga |

### 5. Exemplo típico
Consumir un ficheiro que foi subido previamente a MinIO antes de transformalo con ConvertRecord.

### 6. Boas prácticas
- Non usar FetchS3Object xunto con ListS3 nun único nodo dun cluster (duplicación).
- Usar sempre `ListS3 → FetchS3Object`.

---

## 2.3. ListS3

### 1. Descrición
Lista obxectos dun bucket, **sen descargar contido**, creando un FlowFile por obxecto atopado.

### 2. Función principal
- Descubrir novos ficheiros nun bucket MinIO/S3.
- Detectar cambios para procesamento incremental.

### 3. Propiedades principais
| Propiedade | Tipo | Descrición |
|------------|------|-------------|
| Bucket     | Texto/EL | Nome do bucket |
| Prefix     | Texto | Filtrar por prefixo |
| Minimum Object Age | Tempo | Evitar ficheiros recén subidos |

### 4. Relacións
| Relación | Significado |
|----------|-------------|
| success  | Lista xerada correctamente |

### 5. Exemplo típico
```
ListS3 → FetchS3Object → ConvertRecord → PutHDFS
```

### 6. Boas prácticas
- Ideal para fluxos estilo crawler.
- Manter `State Management = local / cluster` para rastrexar que xa foi procesado.

---

# ================================
# 3. Procesadores HDFS
# ================================

## 3.1. PutHDFS

### 1. Descrición
Escribe FlowFiles en HDFS (Hadoop Distributed File System). É o procesador principal para enviar datos desde NiFi ao teu cluster Hadoop/Spark.

### 2. Función principal
- Crear ficheiros en HDFS.
- Definir rutas dinámicas.
- Crear carpetas automaticamente (opcional).

### 3. Propiedades principais
| Propiedade | Tipo | Descrición |
|------------|------|-------------|
| Directory  | EL   | Ruta en HDFS (ex: `/user/hadoop/nifi/`) |
| Conflict Resolution | Enum | replace / append / fail |
| Compression Codec | Enum | gzip, snappy… (opcional) |

### 4. Relacións
| Relación | Significado |
|----------|-------------|
| success  | Escrita correcta |
| failure  | Erro no HDFS |

### 5. Exemplo típico
```
Directory = /user/hadoop/raw/${now():format('yyyyMMdd')}
Filename = registro-${uuid()}.json
```

### 6. Erros comúns
| Erro | Causa |
|------|-------|
| no such file or directory | Ruta non existe e `Create Directory` está en false |
| permission denied | Permisos Hadoop |

### 7. Boas prácticas
- Separar `raw/`, `clean/` e `curated/` mediante carpetas.
- Usar particionado por data (`yyyy/MM/dd`).

---

## 3.2. ListHDFS

### 1. Descrición
Lista ficheiros existentes nun directorio HDFS. Non descarga o contido.

### 2. Función principal
- Monitorizar directorios HDFS.
- Detectar novos ficheiros cargados por Spark, Sqoop ou outros fluxos.

### 3. Propiedades principais
| Propiedade | Tipo | Descrición |
|------------|------|-------------|
| Directory  | Texto/EL | Directorio base |
| Recurse Subdirectories | Boolean | Buscar recursivamente |

### 4. Relacións
| Relación | Significado |
|----------|-------------|
| success  | Lista xerada |

### 5. Boas prácticas
- Non usar para listar miles de ficheiros nunha pasta enorme.
- Empregar xunto con FetchHDFS cando se queira procesar contido.

---

## 3.3. FetchHDFS

### 1. Descrición
Le o contido dun ficheiro en HDFS e convérteo nun FlowFile.

### 2. Función principal
- Procesar ficheiros dun directorio HDFS.
- Consumir datos xa cargados por Spark, NiFi ou outras ferramentas.

### 3. Propiedades principais
| Propiedade | Tipo | Descrición |
|------------|------|-------------|
| HDFS Filename | Texto/EL | Ruta completa do ficheiro |
| Keep Source File | Boolean | Se false, elimina ficheiro orixinal |

### 4. Relacións
| Relación | Significado |
|----------|-------------|
| success  | Descarga correcta |
| failure  | Non se puido ler |

### 5. Boas prácticas
- Usar tras `ListHDFS` para garantir processado incremental.
- Evitar eliminación automática salvo fluxos controlados.

---

# FIN do documento
