# 03.05 Procesadores Kafka en Apache NiFi

Integración entre NiFi e Kafka: publicación, consumo, formatos, Record API e patróns de deseño.

---

# 1. Por que Kafka + NiFi?

NiFi é excelente para:
- preparar datos antes de envialos a Kafka,
- consumir Kafka e distribuír datos (HDFS, MinIO, APIs…),
- converter formatos con Record API,
- construír pipelines de streaming reais.

---

# 2. PublishKafkaRecord_2_0

## 1. Descrición
Procesador recomendado para publicar datos estruturados en Kafka usando Record API.

## 2. Función principal
- Publicar JSON, CSV, Avro…
- Converter rexistros automaticamente mediante Reader/Writer.
- Enviar eventos a Spark Streaming, Flink…

## 3. Propiedades principais
| Propiedade | Descrición |
|------------|-------------|
| Bootstrap Servers | `kafka:9092` |
| Topic Name | EL permitido |
| Record Reader | JsonTreeReader, CSVReader… |
| Record Writer | JsonRecordSetWriter, Avro… |
| Acks | all / 1 / 0 |
| Partition | `${partition}` opcional |

## 4. Relacións
- success
- failure
- retry

## 5. Exemplo
```
Record Reader = JsonTreeReader
Record Writer = JsonRecordSetWriter
Topic Name = eventos-sensores
```

## 6. Boas prácticas
- Crear o tópico previamente.
- Usar acks=all para datos críticos.
- Non enviar FlowFiles enormes.

---

# 3. PublishKafka_2_0 (non record)

## 1. Descrición
Envia datos en bruto, sen estrutura.

## 2. Cando usalo?
- Logs raw
- Binarios
- Mensaxes sen formato

## 3. Limitacións
- Non transforma nin valida datos.

---

# 4. ConsumeKafkaRecord_2_0

## 1. Descrición
Consume mensaxes Kafka e interprétaas como records mediante un Reader.

## 2. Función principal
- Ler JSON, CSV, Avro…
- Integrarse con ConvertRecord, UpdateRecord…

## 3. Propiedades principais
| Propiedade | Descrición |
|------------|-------------|
| Bootstrap Servers | `kafka:9092` |
| Topic Name | Nome do tópico |
| Group ID | Identificador do consumidor |
| Record Reader | Ex: JsonTreeReader |
| Auto Offset Reset | earliest / latest |

## 4. Relacións
- success
- parse.failure
- failure

## 5. Boas prácticas
- Poñer sempre Group ID.
- earliest para reproducibilidade.
- Splits si hai demasiados rexistros nun FlowFile.

---

# 5. ConsumeKafka_2_0 (non record)

## 1. Descrición
Consume mensaxes raw como FlowFiles.

## 2. Usos típicos
- Logs sen formato.
- Binarios.
- Integración simple NiFi → outro destino.

---

# 6. Seguridade Kafka

## 1. Protocolos soportados
- SASL_PLAINTEXT
- SASL_SSL
- SSL mutuo

## 2. Boas prácticas
- Empregar SSL en contornos produtivos.
- Non usar PLAINTEXT en redes externas.

---

# 7. Patróns de deseño Kafka + NiFi

## 7.1 ETL streaming
```
ConsumeKafkaRecord
    → UpdateRecord
    → ConvertRecord
    → PutHDFS
```

## 7.2 Fan-out
```
ConsumeKafkaRecord
  → RouteOnAttribute
     → PutS3Object
     → PutHDFS
     → PutSQL
```

## 7.3 Normalización antes de publicar
```
HandleHttpRequest
    → ValidateRecord
    → UpdateRecord
    → PublishKafkaRecord
    → HandleHttpResponse
```

## 7.4 Kafka como buffer
```
NiFi → Kafka → Spark Streaming → HDFS
```

---

# 8. Readers e Writers recomendados

## Readers
- JsonTreeReader
- JsonPathReader
- CSVReader
- AvroReader

## Writers
- JsonRecordSetWriter
- AvroRecordSetWriter
- CSVRecordSetWriter

---

# FIN
